{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c1e46ad-1590-4695-99d6-dab328d2a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from metric import get_span\n",
    "import difflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933bcc76-88e3-48a7-9756-9aa12da865f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2617be-fb69-430f-b4fa-7babec14ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f38b085-9601-47a5-acc1-dced0fad1877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bafa0d3-87c7-48d8-9666-35d0c11c4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     model.to('cuda')\n",
    "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be31511-cba1-43d0-bc49-ad91208117df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = tokenizer.mask_token\n",
    "mask_token_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "log_softmax = torch.nn.LogSoftmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4be67a-790b-4de5-8ad6-ada5be36baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"He is a black professor\"\n",
    "sent2 = \"She is a black professor\"\n",
    "sent3 = \"He is a while professor\"\n",
    "sent4 = \"She is a black professor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6486c1cc-055b-4ff9-afef-620481b53060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_token_ids = tokenizer.encode(sent1, return_tensors='pt')\n",
    "sent2_token_ids = tokenizer.encode(sent2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e32574-ddd6-4598-be70-34f05bcb5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1, template2 = get_span(sent1_token_ids[0], sent2_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e57c91e-cba2-4f8e-b0f5-911ee6d10910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob_unigram(masked_token_ids, token_ids, mask_idx, lm):\n",
    "    \"\"\"\n",
    "    Given a sequence of token ids, with one masked token, return the log probability of the masked token.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = lm[\"model\"]\n",
    "    tokenizer = lm[\"tokenizer\"]\n",
    "    log_softmax = lm[\"log_softmax\"]\n",
    "    mask_token = lm[\"mask_token\"]\n",
    "\n",
    "    # get model hidden states\n",
    "    output = model(masked_token_ids)\n",
    "    hidden_states = output[0].squeeze(0)\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "    # we only need log_prob for the MASK tokens\n",
    "    assert masked_token_ids[0][mask_idx] == mask_id\n",
    "\n",
    "    hs = hidden_states[mask_idx]\n",
    "    target_id = token_ids[0][mask_idx]\n",
    "    log_probs = log_softmax(hs)[target_id]\n",
    "\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "536ae115-ee81-453c-9f07-8364ee9cfabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = {\"model\": model,\n",
    "      \"tokenizer\": tokenizer,\n",
    "      \"mask_token\": mask_token,\n",
    "      \"log_softmax\": log_softmax,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b55d0502-9607-4547-a0c3-32ce5d9e16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_log_probs = {}\n",
    "sent2_log_probs = {}\n",
    "total_masked_tokens = 0\n",
    "N = len(template1)\n",
    "\n",
    "# skipping CLS and SEP tokens, they'll never be masked\n",
    "for i in range(1, N-1):\n",
    "    sent1_masked_token_ids = sent1_token_ids.clone().detach()\n",
    "    sent2_masked_token_ids = sent2_token_ids.clone().detach()\n",
    "\n",
    "    sent1_masked_token_ids[0][template1[i]] = mask_token_id\n",
    "    sent2_masked_token_ids[0][template2[i]] = mask_token_id\n",
    "    total_masked_tokens += 1\n",
    "\n",
    "    score1 = get_log_prob_unigram(sent1_masked_token_ids, sent1_token_ids, template1[i], lm)\n",
    "    score2 = get_log_prob_unigram(sent2_masked_token_ids, sent2_token_ids, template2[i], lm)\n",
    "\n",
    "    sent1_log_probs[template1[i]] = score1.item() # += score1.item()\n",
    "    sent2_log_probs[template2[i]] = score2.item() # += score2.item()\n",
    "\n",
    "pair = {\n",
    "    'sent1_token_ids':sent1_token_ids,\n",
    "    'sent2_token_ids':sent2_token_ids,\n",
    "    'sent1_log_probs':sent1_log_probs,\n",
    "    'sent2_log_probs':sent2_log_probs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c217ca39-e141-4705-ae86-c802db3255e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff(pair):\n",
    "    sent1_token_ids = pair['sent1_token_ids'][0]\n",
    "    sent2_token_ids = pair['sent2_token_ids'][0]\n",
    "    sent1_log_probs = pair['sent1_log_probs']\n",
    "    sent2_log_probs = pair['sent2_log_probs']    \n",
    "    \n",
    "    seq1 = [str(x) for x in sent1_token_ids.tolist()]\n",
    "    seq2 = [str(x) for x in sent2_token_ids.tolist()]\n",
    "    dataframes=[]\n",
    "\n",
    "    matcher = difflib.SequenceMatcher(None, seq1, seq2)\n",
    "    template1, template2 = [], []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        a=[tokenizer.ids_to_tokens[int(x)] for x in seq1[i1:i2]]\n",
    "        b=[tokenizer.ids_to_tokens[int(x)] for x in seq2[j1:j2]]\n",
    "        c=[sent1_log_probs.get(x,-np.inf) for x in range(i1,i2)]\n",
    "        d=[sent2_log_probs.get(x,-np.inf) for x in range(j1,j2)]\n",
    "        e=[0]*len(a)\n",
    "        if tag!='equal':\n",
    "            a=tokenizer.decode([int(x) for x in seq1[i1:i2]])\n",
    "            b=tokenizer.decode([int(x) for x in seq2[j1:j2]])\n",
    "            c=[-np.inf]\n",
    "            d=[-np.inf]\n",
    "            e=[1]*len(c)\n",
    "        data = {'token_more':a,\n",
    "                'token_less':b,\n",
    "                'score_more':c,\n",
    "                'score_less':d,\n",
    "                'bias_tokens':e\n",
    "               }\n",
    "        dataframes.append(pd.DataFrame.from_dict(data))\n",
    "        \n",
    "    df = pd.concat(dataframes).reset_index(drop=True)\n",
    "    df = df.assign(prob_more=lambda x: np.exp(x.score_more),\n",
    "                   prob_less=lambda x: np.exp(x.score_less),\n",
    "                   prob_diff=lambda x: x.prob_more-x.prob_less,\n",
    "                   prob_diff_scaled=lambda x: MinMaxScaler().fit_transform(x.prob_diff.values.reshape(-1, 1)))\n",
    "    return df.drop('bias_tokens',axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "579e3b0e-0096-460b-b209-4d01c1631d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_diff(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00e42614-2c15-48ad-9d2b-22f5f8ad3cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_more</th>\n",
       "      <th>token_less</th>\n",
       "      <th>score_more</th>\n",
       "      <th>score_less</th>\n",
       "      <th>prob_more</th>\n",
       "      <th>prob_less</th>\n",
       "      <th>prob_diff</th>\n",
       "      <th>prob_diff_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.177947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he</td>\n",
       "      <td>she</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.177947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>-0.417109</td>\n",
       "      <td>-0.540704</td>\n",
       "      <td>6.589492e-01</td>\n",
       "      <td>5.823379e-01</td>\n",
       "      <td>7.661133e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>-0.069630</td>\n",
       "      <td>-0.052006</td>\n",
       "      <td>9.327392e-01</td>\n",
       "      <td>9.493230e-01</td>\n",
       "      <td>-1.658374e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "      <td>-11.593366</td>\n",
       "      <td>-11.443108</td>\n",
       "      <td>9.227101e-06</td>\n",
       "      <td>1.072313e-05</td>\n",
       "      <td>-1.496028e-06</td>\n",
       "      <td>0.177931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>professor</td>\n",
       "      <td>professor</td>\n",
       "      <td>-18.602179</td>\n",
       "      <td>-17.574081</td>\n",
       "      <td>8.340201e-09</td>\n",
       "      <td>2.331704e-08</td>\n",
       "      <td>-1.497684e-08</td>\n",
       "      <td>0.177946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.177947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token_more token_less  score_more  score_less     prob_more     prob_less  \\\n",
       "0      [CLS]      [CLS]        -inf        -inf  0.000000e+00  0.000000e+00   \n",
       "1         he        she        -inf        -inf  0.000000e+00  0.000000e+00   \n",
       "2         is         is   -0.417109   -0.540704  6.589492e-01  5.823379e-01   \n",
       "3          a          a   -0.069630   -0.052006  9.327392e-01  9.493230e-01   \n",
       "4      black      black  -11.593366  -11.443108  9.227101e-06  1.072313e-05   \n",
       "5  professor  professor  -18.602179  -17.574081  8.340201e-09  2.331704e-08   \n",
       "6      [SEP]      [SEP]        -inf        -inf  0.000000e+00  0.000000e+00   \n",
       "\n",
       "      prob_diff  prob_diff_scaled  \n",
       "0  0.000000e+00          0.177947  \n",
       "1  0.000000e+00          0.177947  \n",
       "2  7.661133e-02          1.000000  \n",
       "3 -1.658374e-02          0.000000  \n",
       "4 -1.496028e-06          0.177931  \n",
       "5 -1.497684e-08          0.177946  \n",
       "6  0.000000e+00          0.177947  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe6e54-5f31-4c8a-a676-fac12eea860a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
